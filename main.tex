%! TeX program = lualatex

\documentclass{fit-teorsem}

%-------------------------------------------------------------------------------
%                 Fill in seminar information
%-------------------------------------------------------------------------------
\lecturername{Ondřej Kvapil}
\lectureremail{kvapiond@fit.cvut.cz}
\papertitle{The Complexity of Interaction}
\paperauthors{Stéphane Gimenez, Georg Moser}
\paperlink{https://dl.acm.org/doi/10.1145/2914770.2837646}

%-------------------------------------------------------------------------------
%                 Use custom packages
%-------------------------------------------------------------------------------
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{tikz}
\usetikzlibrary{arrows,cd,positioning,shapes,fit}

\tikzset{
	encircle/.style = {draw, circle, inner sep = 0.5mm, color = red},
	dot/.style = {circle, minimum size = 1mm, inner sep = 0mm, draw = black},
	reflexive dot/.style={loop,looseness=17,in=130,out=50},
	reflexive above/.style={->,loop,looseness=7,in=120,out=60},
	reflexive below/.style={->,loop,looseness=7,in=240,out=300},
	reflexive left/.style={->,loop,looseness=7,in=150,out=210},
	reflexive right/.style={->,loop,looseness=7,in=30,out=330}
}

\begin{document}
%-------------------------------------------------------------------------------
%                 Print seminar header
%-------------------------------------------------------------------------------
\maketsheader
%-------------------------------------------------------------------------------
%                 Create your content!
%-------------------------------------------------------------------------------
\thispagestyle{empty}

\section*{Notes}
\begin{itemize}
	\item It may be confusing to see that data constructors pass results on
		principal ports, but functions don't use principal ports for outputs
	\item how are interaction nets better than parallel $\beta$-reduction of
		the lambda calculus?
	\item there's a lot of linear logic stuff, how far into the references
		do we want to go?
	\item we should definitely go over the basics of rewriting systems
		(notably explain confluence, aka the diamond property) and
		probably over a brief reminder of what the LC actually is
		\begin{itemize}
			\item we can use \verb|\faDiamond| from the FontAwesome package
				for the diamond property
		\end{itemize}
	\item is the timed sequential reduction \textit{required} to exhibit
		the diamond property, or is that clear from its definition?
	\item regarding $\sqsubset$: so we're working towards \textit{sequential}
		space-time complexity, and now we've defined a tool that
		\textit{sequences functions} with some time delays in-between
	\item what's a convolution?
	\item conventional IRs/ASTs let you reference a node from any number of places.
		This is not true in interaction nets, where the degree of each type of node
		is always fixed -- it is impossible to connect new edges at will
	\item the framework we're building towards lets us reason with implementation
		details, such as the \textit{space occupancy} of individual typed nodes
	\item \textbf{Theorem 1} (Sequential Space-Time Complexity) Associate a function $\gamma_c : T \to S$,
		called \textit{space-time potential}, to every typed node $c$, such that
		$\underbrace{\gamma_c(0) \ge |c|}_{\begin{subarray}{c}
			\text{The space complexity} \\
			\text{of node $c$ at time $0$ is at least} \\
			\text{the space weight of node $c$}
		\end{subarray}}$ and \[
			\underbrace{\overbrace{\left(\coprod_{c \in L} \gamma_c\right)}^{\begin{subarray}{c}
					\text{The convolution} \\
					\text{on the LHS}
				\end{subarray}}\overbrace{(t + d)}^{\begin{subarray}{c}
					\text{at a time $t$ offset} \\
					\text{by the duration $d$} \\
					\text{of the reduction rule}
				\end{subarray}}}_{
			\begin{subarray}{c}
				\text{The maximum space complexity} \\
				\text{of the LHS at time $t + d$}
			\end{subarray}}
				\ge \underbrace{\overbrace{\left(\coprod_{c \in R} \gamma_c\right)}^{\begin{subarray}{c}
						\text{The convolution} \\
						\text{on the RHS}
					\end{subarray}}(t)}_{
				\begin{subarray}{c}
					\text{The maximum space complexity} \\
					\text{of the RHS at time $t$}
				\end{subarray}}
		\] for every reduction rule $L \stackrel{d}{\longrightarrow} R$. Then \[
			N \stackrel{t}{\longrightarrow}_s M
				\Longrightarrow |M| \le \left(\coprod_{c \in N} \gamma_c\right)(t)
		.\]
		In other words, the precondition states that when given the choice to either apply a timed
		reduction rule of duration $d$ at time $t$ or to wait until time $t + d$, the application
		of the rule must not result in an increase in space complexity when compared to the option
		to wait. This condition must hold for all reduction rules, we must choose a space-time
		potential which satisfies it. Additionally, the space-time potential must also be consistent
		with the space weights. \textit{Given these assumptions}, we observe that the timed sequential
		reduction $N \stackrel{t}{\longrightarrow}_s M$ implies a lack of an increase in the space
		occupation ($|M|$) compared to the space-time potential of the unreduced net at time $t$?

		\begin{itemize}
			\item but why would the value of the space-time potential change over time for a net that's not
				undergoing any reductions?
			\item this shines a little bit of light on the choices we can make when creating $\gamma_c$'s.
				There's the option to ``allocate'' more complexity than a node actually needs,
				which could potentially be used for amortised complexity analysis. However, once
				we apply the sequential reduction of duration $t$, we must not end up with a ``larger''
				net than what we had allocated for ($|M|$ could be greater than $|N|$, but this negative
				difference must not exceed the ``plan'' expressed via the space-time potential for $N$).
			\item to sum up, Theorem 1 states that with ``monotonic'' reduction rules the space-time
				potentials can never make the mistake of underestimating the space occupancy of a net.
		\end{itemize}

		Recall the cost model defined earlier:
		\begin{description}
			\item[Cost Model] Given a timed reduction $\longrightarrow$ and a notion of space
				occupation $|\cdot|$ for nets (typically, the number of nodes) in a chosen
				space domain $S$, we say that $N$ admits:
				\begin{itemize}
					\item $\tau \in T$ as a time bound if whenever
						$N \stackrel{t}{\longrightarrow} M$, $t \le \tau$
					\item $\sigma \in S$ as a space bound if whenever
						$N \stackrel{t}{\longrightarrow} M$, $|M| \le \sigma$
					\item $\gamma : T \to S$ as a space-time bound if whenever
						$N \stackrel{t}{\longrightarrow} M$, $|M| \le \gamma(t).$
				\end{itemize}
		\end{description}
		\begin{itemize}
			\item does this mean that the space-time potential (or rather its convolution for the entire net)
				is (weakly) monotonic?
			\item hmm, the system's reduction rules are actually steps of computation. We would like
				the steps to make progress towards a result. But doesn't the monotonicity take
				Turing-completeness away? If we \textit{can} perform complexity analysis on a net
				and get a finite result, that's proof that its evaluation must halt!
			\item in theory, now that we have potentials for all the nodes, it should suffice
				to assign potentials to the reduction rules and prove that those really match
				the theorem (wait, isn't that what we're doing??). Potentials are static things,
				like nets are, the interesting dynamic part is how do the potentials change
				when reduction rules are applied (do the potentials of unrelated nodes stay
				the same?)
				\begin{itemize}
					\item isn't that what we're doing?: actually yeah, the presumption requires
						$\gamma_c$'s even for the RHS, which means we know the potentials in the
						new net
				\end{itemize}
		\end{itemize}
		\begin{description}
			\item[Proof of Theorem 1] The potential-decrease property assumed for reduction rules
				entails the same property for individual sequential reduction steps of a net:
				\begin{enumerate}
					\item $\gamma_L (t + d) \ge \gamma_R (t)$
					\item \begin{align*}
							& (\gamma \ast \gamma_L) (t + d) = \\
							& \max_{u + v = t + d} (\gamma(u) + \gamma_L(v)) \ge \\
							& \max_{u + v = t} (\gamma(u) + \gamma_L(v + d)) \ge \\
							& \max_{u + v = t} (\gamma(u) + \gamma_R(v)) = \\
							& (\gamma \ast \gamma_R)(t)
						.\end{align*}
				\end{enumerate}
		\end{description}
	\item let's look at the size bounds for multiplication:
		\begin{align*}
			\tau_{mul}(n + 1, m) & = \tau_{aux}(m, n) \\
			n = 0 \Rightarrow \tau_{mul}(n, m) & = \tau_{\epsilon}(m) + \tau_{zero}(n m) \\
			\tau_{aux}(m + 1, n) & = \tau_{mul}(n, m + 1) + \tau_{add}(m, n m + n) + \tau_{\delta}(m)
				+ \tau_{succ}(m) + \tau_{succ}(n m + n + m) \\
								 & = \tau_{mul}(n, m + 1) + (m + 1) + (m + 1) + 0 + 0 \\
								 & = \tau_{mul}(n, m + 1) + 2m + 2 \\
								(& = \tau_{aux}(m + 1, n - 1) + 2m + 2) \\
			m = 0 \Rightarrow \tau_{aux}(m, n) & = \tau_{\epsilon}(n) + \tau_{zero}(n m + m) \\
			\cline{1-2}
			\tau_{zero}(n) & = 0 \\
			\tau_{succ}(n) & = 0 \\
			\tau_{add}(n, m) & = n + 1 \\
			\tau_{\delta}(n) & = n + 1 \\
			\tau_{\epsilon}(n) & = n + 1 \\
			\cline{1-2}
			\tau_{mul}(n + 1, m + 1) & = \tau_{mul}(n, m + 1) + 2m + 2 \\
			n = 0 \Rightarrow \tau_{mul}(n, m) & = m + 1 \\
			m = 0 \Rightarrow \tau_{mul}(n, m) & = n + 1 \\
			\text{given constant $m + 1$:} \\
			\tau(n + 1) & = \tau(n) + 2m + 2 \\
			\tau(n) & = (m + 1)(2n + 1)
		\end{align*}

	\item \textbf{important questions}:
		\begin{enumerate}
			\item how the hell did they come up with the presented space/time/space-time bounds?
			\item $zero$ doesn't seem like a constructor of type $\mathsf{nat}^d$, since it doesn't
				produce any other constructors after duration $d$...
		\end{enumerate}
	\item ``if the problem \textit{doesn't} get smaller over time, then there is no upper bound''
	\item don't forget to link back to the section that defines convolutions for sequential complexity,
		it mentions that for parallel reductions, sums suffice.
	\item corollary 4: in other words, the time potential doesn't underestimate the complexity of the
		parallel reduction from $N$ to $M$.
	\item the important difference between this approach and conventional complexity analysis is that
		here, bounds are given for each wire, therefore for both inputs and outputs.
	\item you could think of the nodes as planes or spaceships that \textit{really} want to crash
		into one another, and every time two of them point at each other, they collide.
		The resulting wreckage may be, in some sense, simpler than the two ships (they could be
		annihilated entirely, in fact), or it could turn out that the complexity of the collision
		made the wreckage larger than just two nodes
	\item okay, so the space-time potentials are objects which can be convoluted etc., but why are they
		functions? Isn't the space-time potential of a \textit{static} net or a \textit{static} node
		(static in the sense that they're not changing over time unless we apply reduction relations
		to them!) constant over time? But wait, the delay operator $\sqsubset$ is designed to express
		space occupation changing over time!

		hmm so it \textit{looks like} we're not talking about the space-time potential of a net,
		we're interested in the space-time potential of a computation, in a sense. That's why
		we give the multiplication node such weird bounds, right? Because they don't express
		the space occupancy of \textit{that one single node}, but of the algorithm it represents,
		in a way
	\item ok so the insight with the parallel reduction is that this is a theoretical model, in which
		the application of a rule to a redex \textit{is actually instantaneous}, regardless of any
		delays given for the rule! This doesn't correspond well to the implementation detail
		which actually requires some time to process the reduction (but a reduction is an atomic
		operation). Therefore, the atomicity of reductions is worked around using these $c(r)$ counters,
		which ``pause'' the redex for the required amount of time, and then magically replace it with
		the RHS of the corresponding rule.
\end{itemize}

\section*{Definitions}

\section*{Theorems}

\end{document}
